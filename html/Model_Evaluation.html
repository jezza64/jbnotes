<LINK href="air_style_sheet.css" rel="stylesheet" type="text/css">
<h4><a href="index.html">Back to index</a></h4>
<h1>Evaluating Models</h1>
<h2>Evaluating Binary classifiers</h2>
<h3>Precision recall curves</h3>
<p>X is precision, Y is recall.</p>
<p>Ideal classifier would be perfect for both. Shows trade off between both. Apply varying decision boundary to get the curve. As precision goes up, recall goes down. Jagged: discrete counts used in calcs, so as you get to the edges the discrete counts cause big jumps as goes from 2 to 1.</p>
<p><img src="jbnotes_images/2020-02-16-13-04-05.png" alt=""></p>
<h3>ROC curves</h3>
<p>Receiver operating characteristic curves used for performance of a binary classifier.</p>
<p>X is false positive rate, y is true positive rate. Upper left is perfect. Dotted line is baseline. Look at area underneath the curve to maximise this on ROC curve.</p>
<p><img src="jbnotes_images/2020-02-16-13-11-39.png" alt=""></p>
<h2>Multi class Evaluation</h2>
<p>Collection of true vs predicted binary outcomes, one per class. Confusion matrix for multi class and Classification reports work well.</p>
<p>Various measures to average multi class results, and imbalanced class issues. Also multi label is more complex than multi class evaluation.</p>
<p>Look at confusion matrix for an insight into whats going wrong.</p>
<p><img src="jbnotes_images/2020-02-16-13-23-45.png" alt=""></p>
<p>create with:</p>
<p><img src="jbnotes_images/2020-02-16-13-25-50.png" alt=""></p>
<p>A bad model example: use heatmap to show errors</p>
<p><img src="jbnotes_images/2020-02-16-13-28-58.png" alt=""></p>
<p><img src="jbnotes_images/2020-02-16-13-30-42.png" alt=""></p>
<h3>Macro average precision</h3>
<p>Each class has equal weight. Compute e.g. precision for each class (true positive / ).
Then average over all classes.
If e.g. loads from one class, equal weight with other classes.</p>
<p><img src="jbnotes_images/2020-02-16-13-39-58.png" alt=""></p>
<h3>Micro average precision</h3>
<p>Each instance has equal weight.</p>
<p><img src="jbnotes_images/2020-02-16-13-39-22.png" alt=""></p>
<p><img src="jbnotes_images/2020-02-16-13-41-37.png" alt=""></p>
<h2>Regression Evaluation</h2>
<p>Could look at predictions and categorise the error. But in practice, not so useful.
Typically r2 score is enough.</p>
<h3>R2 (R squared)</h3>
<ul>
<li>computes how well future instances will be predicted</li>
<li>perfect predictor give r squared = 1</li>
<li>constant prediction gives r2 = 0</li>
<li>can go negarive for bad model fits (e.g. non linear models)</li>
</ul>
<h3>Other regression measures</h3>
<ul>
<li>mean_absolute_error. Expected value of L1 norm loss.</li>
<li>mean_squared_error. expected value of L2 norm loss. Widely used. Under and over treated the same.</li>
<li>median_absolute_error (robust to outliers).</li>
</ul>
<h3>Dummy regressors</h3>
<p>provided by scikit learn.
e.g. flat line for one variable vs y
flat, quantile, etc.</p>
